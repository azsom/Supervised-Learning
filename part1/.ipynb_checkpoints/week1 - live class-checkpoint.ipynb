{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mud card and piazza questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final project\n",
    "- look for datasets on the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) or on [Kaggle](https://www.kaggle.com/).\n",
    "- Bring your own dataset!\n",
    "   - if you have your own dataset you'd like to work with, this is the perfect opportunity!\n",
    "- Avoid the most popular datasets! \n",
    "   - no Titanic, no iris for example\n",
    "- avoid these four datasets because we will use them in class and you'll work with them in the problem sets\n",
    "   - [adult dataset](https://archive.ics.uci.edu/ml/datasets/Adult)\n",
    "   - [kaggle house price dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/overview)\n",
    "   - [hand postures dataset](https://archive.ics.uci.edu/ml/datasets/Motion+Capture+Hand+Postures)\n",
    "   - [diabetes dataset](https://scikit-learn.org/stable/datasets/index.html#diabetes-dataset)\n",
    "- work on a classification or regression problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General questions about the course\n",
    "- **\"Since the remote modules count for 10% of our grade, I'm just hoping to clarify how the they will be marked as \"complete\". It doesn't seem like there is any indicator on my side of Canvas that marks a video as complete once I've watched to the end, so I just want to make sure I'm not missing something. It seems to me that the completion of all the quizzes and the Mud Card would be the main indication that we had worked through the modules. Is this assumption correct?\"**\n",
    "   - I see who watched the videos and completed the quizzes and when. Canvas tracks this sort of info. \n",
    "- **\"Will the exams be take-home exams that we will do on our own time, or will we somehow all take the exam synchronously (if that's even possible)? You mention that they are short and open-book, but will they be timed?\"**\n",
    "    - You will have a 12 hour period to start the exam and once you start (i.e., click the github classroom link), you'll have 1 or 1.5 hours to complete it and submit your pdf on gradescope.\n",
    "    - It will be open book.\n",
    "    - I'll send out more info on the exam a couple of weeks before it happens, didn't decide on exam dates yet.\n",
    "- **\"For me, conceptually the pipeline makes sense. I would like to go in depth on more of specific \"how tos\" regarding the code for each steps and what works and what doesn't with regards to that.\"**\n",
    "   - We have at least one week on each step of the ML pipeline so we will go into details and how tos.\n",
    "- **SVM - support vector machines**\n",
    "   - no worries, we will cover this technique in detail around mid/end of October.\n",
    "   - I think you'll learn about it DATA1010 as well.\n",
    "- **I might be struggled with understanding the mathematical intuition of each machine learning model. Hope to have a great chance to learn it, and explore it with practical application in my project**\n",
    "   - We will cover the math behind linear and logistic regression and you'll have an intuitive understandinf of support vector machines and tree-based methods like random forests.\n",
    "   - There is much more math behind these techniques that we won't cover in this class.\n",
    "- **\"Conceptually, I can see how in this module each part built on from the previous and led to the final step (7) interpretation of the data. I was curious/would appreciate more information about the following:, Classification and Regression problems, What other problems exists and what would real example of that look like?, Preprocess the data, Standardization of a dataset - I really just want to understand this in more detail, as from my understanding, this is where the data starts to feed information., If this is not the starting point, please correct me to at what point of the Pipeline does the foundation of our model start?, We worked one algorithm, \"SVC \", in this module and followed it through the pipeline to pin-point its highest accuracy model through step 6, but how are multiple algorithms compared to¬† each other? How do we explore/measure different algorithms to find the best one for the defined evaluation metrics.\"**\n",
    "   - All of these topics will be covered in DATA1030. :) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting\n",
    "\n",
    "- **\"I am struggling with how we split the validation set and how we use the validation set to evaluate the model.\"**\n",
    "- **\"I feel confused about the tuning process.\"**\n",
    "   - No worries, we will spend a whole week on it and I'll explain in detail why it is important and how to do it.\n",
    "\n",
    "- **\"If we were constructing this pipeline to actually use in practice would we want to be more granular with choice of parameter C for our SVM model? For example pick a range of values around the optimum value found by the logspace array to see if we can further reduce overall bias and variance/improve performance on our validation data.\"**\n",
    "   - Yes, you can absolutely do that. In practice however, not much improvement is gained by a more granular grid.\n",
    "   - If you want to improve performance, that's more likely achieved by generating new features or collecting more data.\n",
    "- **\"Why you were splitting some parts of the data by 60/40 and then splitting the remaining 40 in half. Why and how did you choose these numbers / percentages to split the data?\"**\n",
    "   - We will talk about this more in two weeks.\n",
    "   - The 60/20/20 split is used most often on small datasets. Large datasets (with millions of data points) are often split 98/1/1 for example.\n",
    "   \n",
    "- **\"When Split data into different sets and use the random train_test_split method, we are assuming the data to be independent of each other right? If the data is co-related in the data set and we are doing blind model training just like the toy example, how are we going to know that the data is co-related? Maybe we should look into the data first before the cross-validation(for data split)?\"**\n",
    "   - We will do expolatory data analysis (EDA) in the next module to figure out exactly these sort of things about the data. Then week 3 is about splitting based on what we learnt in EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias and variance\n",
    "- **\"Why do low variance models perform poorly on validation sets?\"**\n",
    "   - low variance (or high bias) models are too simple and they don't capture the full complexity of the data\n",
    "      - e.g., fitting a line to a set of points that are clearly non-linear\n",
    "   - as a result, they perform poorly on both the training and validation sets.\n",
    "- **\"Can you please briefly explain the bias-variance trade-off? I am having some trouble differentiating the train and validation sets.\"**\n",
    "- **\"Bias-variance trade off was the muddiest for me\"**\n",
    "   - Most ML algos have some hyperparameters and you need to decide which parameters are the best.\n",
    "   - You also want to know how your model will perform on previously unseen points\n",
    "   - You solve these two problems by splitting your data into train, validation, and test sets.\n",
    "      - for each hyperparameter combo, you train a model and you evaluate how that model performs on the validation set\n",
    "      - you choose the hyperparameter combo that gives the best evaulation score\n",
    "      - Once you find that that model, you'll evaluate it on the test set.\n",
    "         - the test set was not touched before so it serves as our previously unseen data\n",
    "   - bias-variance trade-off\n",
    "      - a model with certain hyperparameters might be too simple (e.g., linear fit to a non-linear problem)\n",
    "         - that's a high bias model\n",
    "         - it performs poorly on both the train and validation sets\n",
    "      - a model with some other hyperparameters might be too complex (e.g., fit a high order polynomial to a set of linear points, it will fit the noise)\n",
    "         - that's a high variance model\n",
    "         - it performs very well on the training set (overfits in fact), but it performs poorly on the validation set (it doesn't generalize to new data points).\n",
    "      - you want to find a good balance between high bias and high variance\n",
    "         - a model that gives the best validation score has the best balance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding questions\n",
    "- **\"Is it standard/required to use variables \"X_train, X_other, y_train, y_other\" and \"X_val, X_test, y_val, y_test\" as seen above? In other words, by setting test_size=0.4, to apply 0.6 correctly, does the function search for 'x_train' and 'y_train', or variables that include 'train'?\"**\n",
    "   - it's standard but you can call your sets in any way you'd like as long as you use them properly.\n",
    "   - `X_train, X_other, y_train, y_other = train_test_split(X,y,test_size=0.4)`\n",
    "      - `X,y` are the inputs, `X_train, X_other, y_train, y_other` are the outputs.\n",
    "      - ```Returns: splitting : list, length=2 * len(arrays), List containing train-test split of inputs.```\n",
    "         - if you give two arrays as input (X, y), the output will contain four arrays.\n",
    "      - the function doesn't look for variables that include 'train', it just expects to see four variables there.\n",
    "         - experiment with the code: try to remove one or add an extra, rename the variables, etc.\n",
    "- **\"I didn't understand the 'fixing the random seed' portion - what exactly do you mean by this? What does 'fixing the random seed' mean?\"**\n",
    "   - You want your code to be reproducable meaning that if you rerun your code, you want to get back the exact same output. This is very important for debugging your code for example.\n",
    "   - Try this:\n",
    "      - remove the line with the random seed and rerun the cell a couple of times. The output will be different on every rerun.\n",
    "- **Is it fair to say that when implementing Linear Regression in Python we always use¬† the statsmodel and sklearn library? Do we ever use something else besides what the example provided?**\n",
    "   - If you use statsmodel or sklearn you are not implementling linear regression. :) We won't use statsmodel in this class.\n",
    "   - You can use other packages in your project.\n",
    "   - I'll ask you to do stuff we didn't cover in class in problem sets.\n",
    "- **\"Why does \"step size in the mesh\" mean? 2. What's the purpose of np.meshgrid? Could we use any other functions to plot instead?\"**\n",
    "\n",
    "<center><img src=\"figures/decision_boundary.jpg\" width=\"600\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **\"I have done some machine learning problems before, but I used tensorflow instead. Could you please intrepret the framework \"scikit-learn\" a little bit more. Are we required to use scikit-learn in our project?\"**\n",
    "   - Tensorflow is adeep learning package, it is used to train neural networks. \n",
    "   - scikit-learn is a more general package for machine learning. It has a ton of tools to process data, create ML pipelines, many evaluation metrics, bunch of ML algorithms are implemented too.\n",
    "   - You will learn how to use scikit-learn during this course and you are required to use it in your project.\n",
    "   - We will not do any deep learning in DATA1030 because you'll have a dedicated deep learning course next semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **\"I'm interested in the differences between traditional coding pipelines vs machine learning pipelines. Are data scientists ever responsible for writing novel ML algorithms from scratch, or do they mostly apply ML algorithms written by others?\"**\n",
    "   - By far most practicioners apply ML algorithms written by professional software developers (e.g., sklearn, keras, tensorflow, XGBoost).\n",
    "   - New algorithms are developed too especially in deep learning. You need a deep knowledge of mathematics and especially linear algebra and calculus to understand how the algorithms work and to develop novel methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **How do I use the toy data set in binder to recreate what you did?**\n",
    "   - No clue, I never used binder. :D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Any other questions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
