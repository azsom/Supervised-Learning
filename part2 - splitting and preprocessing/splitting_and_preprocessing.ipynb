{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Welcome to Supervised Learning</center>\n",
    "## <center>Part 2: How to prepare your data for supervised machine learning</center>\n",
    "## <center>Instructor: Andras Zsom</center>\n",
    "### <center>https://github.com/azsom/Supervised-Learning<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The topic of the course series: supervised Machine Learning (ML)\n",
    "- how to build an ML pipeline from beginning to deployment\n",
    "- we assume you already performed data cleaning\n",
    "- this is the first course out of 6 courses\n",
    "    - Part 1: Introduction to machine learning and the bias-variance tradeoff\n",
    "    - **Part 2: How to prepare your data for supervised machine learning**\n",
    "    - Part 3: Evaluation metrics in supervised machine learning\n",
    "    - Part 4: SVMs, Random Forests, XGBoost\n",
    "    - Part 5: Missing data in supervised ML\n",
    "    - Part 6: Interpretability\n",
    "- you can complete the courses in sequence or complete individual courses based on your interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured data\n",
    "| X|feature_1|feature_2|...|feature_j|...|feature_m|<font color='red'>Y</font>|\n",
    "|-|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "|__data_point_1__|x_11|x_12|...|x_1j|...|x_1m|__<font color='red'>y_1</font>__|\n",
    "|__data_point_2__|x_21|x_22|...|x_2j|...|x_2m|__<font color='red'>y_2</font>__|\n",
    "|__...__|...|...|...|...|...|...|__<font color='red'>...</font>__|\n",
    "|__data_point_i__|x_i1|x_i2|...|x_ij|...|x_im|__<font color='red'>y_i</font>__|\n",
    "|__...__|...|...|...|...|...|...|__<font color='red'>...</font>__|\n",
    "|__data_point_n__|x_n1|x_n2|...|x_nj|...|x_nm|__<font color='red'>y_n</font>__|\n",
    "\n",
    "We focus on the feature matrix (X) in this course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning objectives of this course\n",
    "\n",
    "By the end of the course, you will be able to\n",
    "- describe why data splitting is necessary in machine learning\n",
    "- summarize the properties of IID data\n",
    "- list examples of non-IID datasets\n",
    "- apply IID splitting techniques\n",
    "- apply non-IID splitting techniques\n",
    "- identify when a custom splitting strategy is necessary\n",
    "- describe the two motivating concepts behind preprocessing\n",
    "- apply various preprocessors to categorical and continuous features\n",
    "- perform preprocessing with a sklearn pipeline and ColumnTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Split IID data\n",
    "### Learning objectives of this module:\n",
    "- describe why data splitting is necessary in machine learning\n",
    "- summarize the properties of IID data\n",
    "- apply IID splitting techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why do we split the data?\n",
    "- we want to find the best hyper-parameters of our ML algorithms\n",
    "   - fit models to training data\n",
    "   - evaluate each model on validation set\n",
    "   - we find hyper-parameter values that optimize the validation score\n",
    "- we want to know how the model will perform on previously unseen data - the generalization error\n",
    "   - apply our final model on the test set\n",
    "   \n",
    "### We need to split the data into three parts!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ask yourself these questions!\n",
    "- What is the intended use of the model? What is it supposed to do/predict?\n",
    "- What data/info do you have available at the time of prediction?\n",
    "- Your split must mimic the intended use of the model only then will you accurately estimate how well the model will perform on previously unseen points (generalization error).\n",
    "- two examples:\n",
    "    - if you want to predict the outcome of a new patient's visit to the ER:\n",
    "        - your test score must be based on patients not included in training and validation\n",
    "        - your validation score must be based on patients not included in training\n",
    "        - points of one patient should not be distributed over multiple sets because your generalization error will be off\n",
    "    - a youtube video was released 4 weeks ago and you want to predict if it will be featured a week from now, your training data should only contain info that will be available when you make predictions (stuff you know 4 weeks after release)\n",
    "        - split data based on youtube vid ID\n",
    "        - use info that's available 4 weeks after release\n",
    "        - your classification label will be whether it was featured or not 5 weeks after release"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How should we split the data into train/validation/test?\n",
    "\n",
    "- data is **Independent and Identically Distributed** (iid)\n",
    "   - all samples stem from the same generative process and the generative process is assumed to have no memory of past generated samples\n",
    "   - identify cats and dogs on images\n",
    "   - predict the house price\n",
    "   - predict if someone's salary is above or below 50k\n",
    "- examples of not iid data:\n",
    "   - data generated by time-dependent processes\n",
    "   - data has group structure (samples collected from e.g., different subjects, experiments, measurement devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Splitting strategies for iid data: basic approach\n",
    "- 60% train, 20% validation, 20% test for small datasets\n",
    "- 98% train, 1% validation, 1% test for large datasets\n",
    "    - if you have 1 million points, you still have 10000 points in validation and test which is plenty to assess model performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Let's work with the adult data!\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/adult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "df = pd.read_csv('data/adult_data.csv')\n",
    "\n",
    "# let's separate the feature matrix X, and target variable y\n",
    "y = df['gross-income'] # remember, we want to predict who earns more than 50k or less than 50k\n",
    "X = df.loc[:, df.columns != 'gross-income'] # all other columns are features\n",
    "print(y)\n",
    "print(X.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "# first split to separate out the training set\n",
    "X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,random_state=random_state)\n",
    "print('training set:',X_train.shape, y_train.shape) # 60% of points are in train\n",
    "print(X_other.shape, y_other.shape) # 40% of points are in other\n",
    "\n",
    "# second split to separate out the validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,random_state=random_state)\n",
    "print('validation set:',X_val.shape, y_val.shape) # 20% of points are in validation\n",
    "print('test set:',X_test.shape, y_test.shape) # 20% of points are in test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomness due to splitting\n",
    "- the model performance, validation and test scores will change depending on which points are in train, val, test\n",
    "    - inherent randomness or uncertainty of the ML pipeline\n",
    "- change the random state a couple of times and repeat the whole ML pipeline to assess how much the random splitting affects your test score\n",
    "    - you would expect a similar uncertainty when the model is deployed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Splitting strategies for iid data: k-fold splitting\n",
    "\n",
    "<center><img src=\"figures/grid_search_cross_validation.png\" width=\"600\"></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "help(KFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "# first split to separate out the test set\n",
    "X_other, X_test, y_other, y_test = train_test_split(X,y,test_size = 0.2,random_state=random_state)\n",
    "print(X_other.shape,y_other.shape)\n",
    "print('test set:',X_test.shape,y_test.shape)\n",
    "\n",
    "# do KFold split on other\n",
    "kf = KFold(n_splits=5,shuffle=True,random_state=random_state)\n",
    "for train_index, val_index in kf.split(X_other,y_other):\n",
    "    X_train = X_other.iloc[train_index]\n",
    "    y_train = y_other.iloc[train_index]\n",
    "    X_val = X_other.iloc[val_index]\n",
    "    y_val = y_other.iloc[val_index]\n",
    "    print('   training set:',X_train.shape, y_train.shape) \n",
    "    print('   validation set:',X_val.shape, y_val.shape) \n",
    "    # the validation set contains different points in each iteration\n",
    "    print(X_val[['age','workclass','education']].head())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How many splits should I create?\n",
    "- tough question, 3-5 is most common\n",
    "- if you do $n$ splits, $n$ models will be trained, so the larger the $n$, the most computationally intensive it will be to train the models\n",
    "- KFold is usually better suited for small datasets\n",
    "- KFold is good to estimate uncertainty due to random splitting of train and val, but it is not perfect\n",
    "    - the test set remains the same"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Why shuffling iid data is important?\n",
    "- by default, data is not shuffled by Kfold which can introduce errors!\n",
    "<center><img src=\"figures/kfold.png\" width=\"600\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalanced data\n",
    "- imbalanced data: only a small fraction of the points are in one of the classes, usually ~5% or less but there is no hard limit here\n",
    "- examples:\n",
    "    - people visit a bank's website. do they sign up for a new credit card?\n",
    "        - most customers just browse and leave the page\n",
    "        - usually 1% or less of the customers get a credit card (class 1), the rest leaves the page without signing up (class 0).\n",
    "    - fraud detection\n",
    "        - only a tiny fraction of credit card payments are fraudulent\n",
    "    - rare disease diagnosis\n",
    "- the issue with imbalanced data:\n",
    "    - if you apply train_test_split or KFold, you might not have class 1 points in one of your sets by chance\n",
    "    - this is what we need to fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution: stratified splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "\n",
    "X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,random_state=random_state)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,random_state=random_state)\n",
    "\n",
    "print('**balance without stratification:**')\n",
    "# a variation on the order of 1% which would be too much for imbalanced data!\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_val.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))\n",
    "\n",
    "X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,stratify=y,random_state=random_state)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,stratify=y_other,random_state=random_state)\n",
    "print('**balance with stratification:**')\n",
    "# very little variation (in the 4th decimal point only) which is important if the problem is imbalanced\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(y_val.value_counts(normalize=True))\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stratified folds\n",
    "<center><img src=\"figures/stratified_kfold.png\" width=\"600\"></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "help(StratifiedKFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what we did before: variance in balance on the order of 1%\n",
    "random_state = 42\n",
    "\n",
    "X_other, X_test, y_other, y_test = train_test_split(X,y,test_size = 0.2,random_state=random_state)\n",
    "print('test balance:',y_test.value_counts(normalize=True))\n",
    "\n",
    "# do KFold split on other\n",
    "kf = KFold(n_splits=5,shuffle=True,random_state=random_state)\n",
    "for train_index, val_index in kf.split(X_other,y_other):\n",
    "    X_train = X_other.iloc[train_index]\n",
    "    y_train = y_other.iloc[train_index]\n",
    "    X_val = X_other.iloc[val_index]\n",
    "    y_val = y_other.iloc[val_index]\n",
    "    print('train balance:')\n",
    "    print(y_train.value_counts(normalize=True))\n",
    "    print('val balance:')\n",
    "    print(y_val.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified K Fold: variation in balance is very small (4th decimal point)\n",
    "random_state = 42\n",
    "\n",
    "# stratified train-test split\n",
    "X_other, X_test, y_other, y_test = train_test_split(X,y,test_size = 0.2,stratify=y,random_state=random_state)\n",
    "print('test balance:',y_test.value_counts(normalize=True))\n",
    "\n",
    "# do StratifiedKFold split on other\n",
    "kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=random_state)\n",
    "for train_index, val_index in kf.split(X_other,y_other):\n",
    "    X_train = X_other.iloc[train_index]\n",
    "    y_train = y_other.iloc[train_index]\n",
    "    X_val = X_other.iloc[val_index]\n",
    "    y_val = y_other.iloc[val_index]\n",
    "    print('train balance:')\n",
    "    print(y_train.value_counts(normalize=True))\n",
    "    print('val balance:')\n",
    "    print(y_val.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 2: Split non-IID data\n",
    "### Learning objectives of this module:\n",
    "- list examples of non-IID datasets\n",
    "- apply non-IID splitting techniques\n",
    "- identify when a custom splitting strategy is necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of non-iid data\n",
    "- if there is any sort of time or group structure in your data, it is likely non-iid\n",
    "    - group structure:\n",
    "        - each point is someone's visit to the ER and some people visited the ER multiple times\n",
    "        - each point is stats of a youtube video and the stats are collected weekly, one of the stats is whether it is featured\n",
    "        - each point is a customer's visit to CVS and customers tend to return regularly\n",
    "    - time structure\n",
    "        - each point is the stocks price at a given time\n",
    "        - eahc point is a person's health or activity status\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group-based split: GroupShuffleSplit\n",
    "<center><img src=\"figures/groupshufflesplit.png\" width=\"600\"></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "X = np.ones(shape=(8, 2))\n",
    "y = np.ones(shape=(8, 1))\n",
    "groups = np.array([1, 1, 2, 2, 2, 3, 3, 3])\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=10, train_size=.8, random_state=42)\n",
    "\n",
    "for train_idx, test_idx in gss.split(X, y, groups):\n",
    "    print(\"TRAIN:\", train_idx, \"TEST:\", test_idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group-based split: GroupKFold\n",
    "<center><img src=\"figures/groupkfold.png\" width=\"600\"></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "group_kfold = GroupKFold(n_splits=3)\n",
    "\n",
    "for train_index, test_index in group_kfold.split(X, y, groups):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(GroupKFold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data leakage in time series data is similar!\n",
    "- do NOT use information in validation or test which will not be available once your model is deployed\n",
    "   - don't use future information!\n",
    "   \n",
    "<center><img src=\"figures/timeseriessplit.png\" width=\"600\"></center>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "tscv = TimeSeriesSplit()\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 3: Preprocess continuous and categorical features\n",
    "### Learning objectives of this module:\n",
    "- describe the two motivating concepts behind preprocessing\n",
    "- apply various preprocessors to categorical and continuous features\n",
    "- perform preprocessing with a sklearn pipeline and ColumnTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data almost never comes in a format that's directly usable in ML\n",
    "- ML works with numerical data but some columns can be text (e.g., home country, educational level, gender, race)\n",
    "    - some ML algorithms accept (and prefer) a non-numerical feature matrix (like [CatBoost](https://catboost.ai/) ) but that's not standard\n",
    "    - sklearn throws an error message if the feature matrix contains non-numerical elements\n",
    "- the order of magnitude of numerical features can vary greatly which is not good for most ML algorithms (e.g., salary in USD, age in years, time spent on the site in sec)\n",
    "    - many ML algorithms are distance-based and they perform better and converge faster if the features are standardized (features have a mean of 0 and the same standard deviation, usually 1)\n",
    "        - Lasso and Ridge regression because of the penalty term, K Nearest Neightbors, SVM, linear models if you want to use the coefficients to measure feature importance (more on this in part 6), neural networks\n",
    "    - tree-based methods don't require standardization \n",
    "    - check out part 1 to learn more about linear and logistic regression, Lasso and Ridge\n",
    "    - check out part 4 to learn more about SVMs, tree-based methods, and K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### scikit-learn transformers to the rescue!\n",
    "\n",
    "Preprocessing is done with various transformers. All transformes have three methods:\n",
    "- **fit** method: estimates parameters necessary to do the transformation,\n",
    "- **transform** method: transforms the data based on the estimated parameters,\n",
    "- **fit_transform** method: both steps are performed at once, this can be faster than doing the steps separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Transformers we cover \n",
    "- **OrdinalEncoder** - converts categorical features into an integer array\n",
    "- **OneHotEncoder** - converts categorical features into dummy arrays\n",
    "- **StandardScaler** - standardizes continuous features by removing the mean and scaling to unit variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ordered categorical data: OrdinalEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we have a categorical feature and training and test sets\n",
    "\n",
    "The cateogies can be ordered or ranked\n",
    "\n",
    "E.g., educational level in the adult dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_edu = {'educational level':['Bachelors','Masters','Bachelors','Doctorate','HS-grad','Masters']} \n",
    "test_edu = {'educational level':['HS-grad','Masters','Masters','College','Bachelors']}\n",
    "\n",
    "X_train = pd.DataFrame(train_edu)\n",
    "X_test = pd.DataFrame(test_edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "help(OrdinalEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# initialize the encoder\n",
    "cats = ['HS-grad','Bachelors','Masters','Doctorate']\n",
    "\n",
    "enc = OrdinalEncoder(categories = [cats]) # The ordered list of \n",
    "# categories need to be provided. By default, the categories are alphabetically ordered!\n",
    "\n",
    "# fit the training data\n",
    "enc.fit(X_train)\n",
    "# print the categories - not really important because we manually gave the ordered list of categories\n",
    "print(enc.categories_)\n",
    "# transform X_train. We could have used enc.fit_transform(X_train) to combine fit and transform\n",
    "X_train_oe = enc.transform(X_train)\n",
    "print(X_train_oe)\n",
    "# transform X_test\n",
    "X_test_oe = enc.transform(X_test) # OrdinalEncoder always throws an error message if \n",
    "                                  # it encounters an unknown category in test\n",
    "print(X_test_oe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unordered categorical data: one-hot encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some categories cannot be ordered. e.g., workclass, relationship status\n",
    "\n",
    "first feature: gender (male, female, unknown)\n",
    "\n",
    "second feature: browser  used \n",
    "\n",
    "these categories cannot be ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {'gender':['Male','Female','Unknown','Male','Female','Female'],\\\n",
    "         'browser':['Safari','Safari','Internet Explorer','Chrome','Chrome','Internet Explorer']}\n",
    "test = {'gender':['Female','Male','Unknown','Female'],'browser':['Chrome','Firefox','Internet Explorer','Safari']}\n",
    "\n",
    "X_train = pd.DataFrame(train)\n",
    "X_test = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# How do we convert this to numerical features?\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "help(OneHotEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# initialize the encoder\n",
    "enc = OneHotEncoder(sparse=False) # by default, OneHotEncoder returns a sparse matrix. sparse=False returns a 2D array\n",
    "# fit the training data\n",
    "enc.fit(X_train)\n",
    "print('categories:',enc.categories_)\n",
    "print('feature names:',enc.get_feature_names())\n",
    "# transform X_train\n",
    "X_train_ohe = enc.transform(X_train)\n",
    "#print(X_train_ohe)\n",
    "# do all of this in one step\n",
    "X_train_ohe = enc.fit_transform(X_train)\n",
    "#print(X_train_ohe)\n",
    "\n",
    "# transform X_test\n",
    "X_test_ohe = enc.transform(X_test)\n",
    "print('X_test transformed')\n",
    "print(X_test_ohe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Continuous features: StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = {'salary':[50_000,75_000,40_000,1_000_000,30_000,250_000,35_000,45_000]}\n",
    "test = {'salary':[25_000,55_000,1_500_000,60_000]}\n",
    "\n",
    "X_train = pd.DataFrame(train)\n",
    "X_test = pd.DataFrame(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "help(StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "print(scaler.fit_transform(X_train))\n",
    "print(scaler.transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How and when to do preprocessing in the ML pipeline?\n",
    "- **SPLIT YOUR DATA FIRST!**\n",
    "- **APPLY TRANSFORMER.FIT ONLY ON YOUR TRAINING DATA!** Then transform the validation and test sets.\n",
    "- One of the most common mistake practitioners make is leaking statistics!\n",
    "     - fit_transform is applied to the whole dataset, then the data is split into train/validation/test\n",
    "         - this is wrong because the test set statistics impacts how the training and validation sets are transformed\n",
    "         - but the test set must be separated from train and val, and val must be separated from train\n",
    "     - or fit_transform is applied to the train, then fit_transform is applied to the validation set, and fit_transform is applied to the test set\n",
    "         - this is wrong because the relative position of the points change\n",
    "<center><img src=\"figures/no_separate_scaling.png\" width=\"1200\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Scikit-learn's pipelines\n",
    "\n",
    "- Preprocessing and model training (not the splitting) can be chained together into a scikit-learn pipeline which consists of transformers and one final estimator which is usually your classifier or regression model.\n",
    "- It neatly combines the preprocessing steps and it helps to avoid leaking statistics.\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/compose/plot_column_transformer_mixed_types.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "df = pd.read_csv('data/adult_data.csv')\n",
    "\n",
    "# let's separate the feature matrix X, and target variable y\n",
    "y = df['gross-income'] # remember, we want to predict who earns more than 50k or less than 50k\n",
    "X = df.loc[:, df.columns != 'gross-income'] # all other columns are features\n",
    "\n",
    "random_state = 42\n",
    "\n",
    "# first split to separate out the training set\n",
    "X_train, X_other, y_train, y_other = train_test_split(X,y,train_size = 0.6,random_state=random_state)\n",
    "\n",
    "# second split to separate out the validation and test sets\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_other,y_other,train_size = 0.5,random_state=random_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect which encoder to use on each feature\n",
    "# needs to be done manually\n",
    "ordinal_ftrs = ['education'] \n",
    "ordinal_cats = [[' Preschool',' 1st-4th',' 5th-6th',' 7th-8th',' 9th',' 10th',' 11th',' 12th',' HS-grad',\\\n",
    "                ' Some-college',' Assoc-voc',' Assoc-acdm',' Bachelors',' Masters',' Prof-school',' Doctorate']]\n",
    "onehot_ftrs = ['workclass','marital-status','occupation','relationship','race','sex','native-country']\n",
    "std_ftrs = ['capital-gain','capital-loss','age','hours-per-week']\n",
    "\n",
    "# collect all the encoders\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('ord', OrdinalEncoder(categories = ordinal_cats), ordinal_ftrs),\n",
    "        ('onehot', OneHotEncoder(sparse=False,handle_unknown='ignore'), onehot_ftrs),\n",
    "        ('std', StandardScaler(), std_ftrs)])\n",
    "\n",
    "# for now we only preprocess, later on we will add other steps here\n",
    "# note the final scaler which is a standard scaler\n",
    "# the ordinal and one hot encoded features do not have a mean of 0 and an std of 1\n",
    "# the final scaler standardizes those features\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor),('final scaler',StandardScaler())]) \n",
    "\n",
    "X_train_prep = clf.fit_transform(X_train)\n",
    "X_val_prep = clf.transform(X_val)\n",
    "X_test_prep = clf.transform(X_test)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_train_prep.shape)\n",
    "\n",
    "print(np.mean(X_train_prep,axis=0))\n",
    "print(np.std(X_train_prep,axis=0))\n",
    "print(np.mean(X_val_prep,axis=0))\n",
    "print(np.std(X_val_prep,axis=0))\n",
    "print(np.mean(X_test_prep,axis=0))\n",
    "print(np.std(X_test_prep,axis=0))\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
